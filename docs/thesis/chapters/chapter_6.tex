\chapter{Experimental evaluation}\label{chapter:experimental_evaluation}

In this chapter we propose a set of experiments to study the performance and scalability of both our distributed framework for CER and our distributed evaluation algorithm. This study aims to show that the claims of our work empirically hold. This chapter is organized as follows. First, we describe our implementation. Then, we describe the design and set up of our experiments, including how we generated the synthetic data and the characteristics of the system. Then, for each experiment, we describe the experiment and discuss the results. Finally, we summarise the main conclusions of the chapter.

\section{DCORE in a nutshell}\label{chapter:dcore}

In this section, we review some implementation details of DCORE. We implemented DCORE to run on the JVM \cite{jvm}. Its code is open-source and available at \url{https://github.com/dtim-upc/DCORE} under the GNU GPLv3 license. DCORE implementation depends on a fork of \href{https://github.com/dtim-upc/CORE2/tree/distributed_enumeration}{CORE}. This fork implements our novel distributed evaluation algorithm.

\textbf{CORE.} CORE is implemented in Java 11 and has some clever optimizations. For example, complex predicates are encoded in an efficient array representation and are only evaluated once during the execution of the algorithm. Another example is I/O-determinization which is run \emph{on the fly} are many of the steps are cached and only computed once. Another example is memory management. Nodes in the tECS data structure are weakly referenced, while the strong references are stored in a list that is pruned once in a while taking into account nodes that are outside the time window, allowing the garbage collector to reclaim that memory without the need to modify the tECS data structure.

\textbf{DCORE.} DCORE is implemented in Scala 2.12 \cite{scala}. Scala \cite{scala} is a statically typed programming language that fuses object-oriented and functional programming, which can freely interoperate with Java. DCORE is built on top of Akka \cite{akka} (Akka Cluster in particular). Akka is a set of open-source libraries for designing scalable, resilient system that span processor cores and networks. Akkaâ€™s use of the actor model provides a level of abstraction that makes it easier to write correct concurrent, parallel and distributed systems. DCORE depend on several libraries that are downloaded and compiled using Sbt \cite{sbt}. Sbt is a typesafe and parallel build tool for Scala and Java projects. Sbt guarantees reproducibility of the compilation of the project. As mentioned before, DCORE depends on our own fork of CORE that implements the novel distributed evaluation algorithm. DCORE is built as a Command-line interface program and has many available parameters. The landing page of the project has a detailed explanation on how to compile and run the project. We remark that we made an effort to guarantee that the implementation is correct and outputs the expected result. We designed several automated tests to validate that the implementation is close to the specification.

\section{Experimental setup}\label{sec:setup}

% In this section we describe the common settings to all the experiments and refer to each experiment for its particularities.

We have compared our framework's implementation DCERE and DCORE against the leading CER system: CORE \cite{core}.
We do not compare experimentally against other proposals (e.g., SASE \cite{sase}, Esper \cite{esper}, FlinkCEP \cite{flink-cep}, TESLA/T-REX \cite{tesla}) because CORE throughput is three orders of magnitudes higher than the rest of CER systems serving as a baseline to overcome. CORE, DCERE, and CORE are semantically comparable since they all use the same query language. Unfortunately, CORE cannot evaluate non-unary predicates, so we had to extend the system with an ad-hoc rewrite and refine algorithms, similar to how DCERE is implemented, but without the distributed part.

\textbf{Hardware.} We run our experiments on a server equipped with a 6-core (2 threads per core) i7-8700 processor running at stock frequency, 32GB of RAM, Arch Linux operating system, 5.15.7-arch1-1 kernel version, OpenJDK Runtime 11.0.12, and the OpenJDK 64-Bit Server Virtual Machine build 11.0.12. The Virtual Machine is restarted with 1024MB of freshly allocated memory before each experiment.

\textbf{Note.} All the experiments are run on a single server instead of a cluster of servers. We speculate that the results of our experiments in a cluster would be similar, with small differences due to the overhead of network communication. Validating this hypothesis is left for future work.

We compare systems with respect to their performance. All reported numbers are averages taken over three repetitions of each experiment. We measure the performance of each system, expressed as the execution time of the experiment in milliseconds, as follows. To avoid measuring the data loading time of each system, we first load the input stream completely in main memory. We then start the timer, run the experiment, and stop the timer as soon as the last output is enumerated. Recognized complex events are written to /dev/null device to guarantee that the events are enumerated (avoiding \emph{dead call optimization}), but at the same time not measuring writing costs. The \emph{null device} is a virtual device present in all Linux systems that results in zero cost on writing operations. For consistency reasons, we have verified that all systems produced the same set of complex events.

\textbf{ACM SIGMOD 2022 Availability \& Reproducibility}. Our work complies with ACM SIGMOD 2022 Availability \& Reproducibility criteria \cite{acm-reprodocubility} : (1) our prototype is provided as a white box, (2) the process to generate the input data is available, (3) the experiments can be reproduced in order to generate the experimental data, and (4) the script/spreadsheet to transform the raw data into the plots is provided.

As far as we are concerned, there does not exist a standard benchmark for complex event recognition. For this reason, we experiment with a set of queries over synthetic data. We considered sequence queries and iteration queries for our experiments, which have been used for benchmarking in CER before (for example \cite{cayuga}~and~\cite{experiment-example}). Specifically, we have considered the following three queries:

\begin{figure}[H]
  \centering
  \begin{subfigure}[c]{0.49\textwidth}
    \centering
    \begin{minted}[fontsize=\footnotesize, linenos=false, autogobble]{text}
      SELECT *
      FROM S
      WHERE P
      FILTER A[id] = B[id]
    \end{minted}
  \end{subfigure}
  \begin{subfigure}[t]{0.49\textwidth}
    \begin{tabular}{l l}
      \hline
      $Q_{1}:$ & $P = A;B;C$ \\
      \hline
      $Q_{2}:$ & $P = A;B+;C$ \\
      \hline
      $Q_{3}:$ & $P = A+;B+;C$ \\
      \hline
    \end{tabular}
  \end{subfigure}
\end{figure}

% Talk about all queries contain binary predicate.


\section{Experiments on the evaluation of complex predicates}\label{sec:predicates}

% Subfigure and place them together ?

% A;B;C
\begin{figure}[H]
  \centering
  \includegraphics[scale=0.5]{experiment_1_chart_3}
  \caption{$Q_{1}$.}
  \label{fig:???}
\end{figure}

% A;B+;C
\begin{figure}[H]
  \centering
  \includegraphics[scale=0.5]{experiment_1_chart_2}
  \caption{$Q_{2}$.}
  \label{fig:???}
\end{figure}

% A+;B+;C
\begin{figure}[H]
  \centering
  \includegraphics[scale=0.5]{experiment_1_chart_1}
  \caption{$Q_{3}$.}
  \label{fig:???}
\end{figure}


\textbf{Experimental results}.

\section{Experiments on the scalability of the framework}\label{sec:scalability}

% A;B;C
\begin{figure}[H]
  \centering
  \includegraphics[scale=0.5]{experiment_2_chart_3}
  \caption{$Q_{1}$.}
  \label{fig:???}
\end{figure}

% A;B+;C
\begin{figure}[H]
  \centering
  \includegraphics[scale=0.5]{experiment_2_chart_2}
  \caption{$Q_{2}$.}
  \label{fig:???}
\end{figure}

% A+;B+;C
\begin{figure}[H]
  \centering
  \includegraphics[scale=0.5]{experiment_2_chart_1}
  \caption{$Q_{3}$.}
  \label{fig:???}
\end{figure}



\begin{figure}[H]
  \centering
  \includegraphics[scale=0.5]{experiment_3_chart_1}
  \caption{$Q_{2}$.}
  \label{fig:???}
\end{figure}

\begin{figure}[H]
  \centering
  \includegraphics[scale=0.5]{experiment_3_chart_2}
  \caption{$Q_{2}$.}
  \label{fig:???}
\end{figure}

\textbf{Experimental results}.

\section{Experiments on the novel evaluation algorithm}\label{sec:new-algorithm}

% A;B;C
\begin{figure}[H]
  \centering
  \includegraphics[scale=0.5]{experiment_4_chart_3}
  \caption{$Q_{1}$.}
  \label{fig:???}
\end{figure}
% A;B+;C
\begin{figure}[H]
  \centering
  \includegraphics[scale=0.5]{experiment_4_chart_2}
  \caption{$Q_{2}$.}
  \label{fig:???}
\end{figure}
% A+;B+;C
\begin{figure}[H]
  \centering
  \includegraphics[scale=0.5]{experiment_4_chart_3}
  \caption{$Q_{3}$.}
  \label{fig:???}
\end{figure}


\begin{figure}[H]
  \centering
  \includegraphics[scale=0.5]{experiment_5_chart_1}
  \caption{.}
  \label{fig:???}
\end{figure}

% 2 processors
\begin{figure}[H]
  \centering
  \includegraphics[scale=0.5]{experiment_5_chart_2}
  \caption{.}
  \label{fig:???}
\end{figure}

% 4 processors
\begin{figure}[H]
  \centering
  \includegraphics[scale=0.5]{experiment_5_chart_3}
  \caption{.}
  \label{fig:???}
\end{figure}

% 6 processors
\begin{figure}[H]
  \centering
  \includegraphics[scale=0.5]{experiment_5_chart_4}
  \caption{.}
  \label{fig:???}
\end{figure}

% 8 processors
\begin{figure}[H]
  \centering
  \includegraphics[scale=0.5]{experiment_5_chart_5}
  \caption{.}
  \label{fig:???}
\end{figure}

\textbf{Experimental results}.

\section{Chapter summary}
