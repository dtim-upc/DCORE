\chapter{Related Work}\label{chapter:related_work}

\section{CER systems}\label{sec:cer-systems}

CER systems are usually divided into three categories:  automata-based, tree-based, and logic-based, with some systems (e.g. \cite{esper, tesla}) being hybrids. We situate our work among the three approaches, and refer the reader to recent surveys \cite{survey-systems-1,survey-systems-2,research-evaluation-query} for an in-depth discussion of these classes of systems.

\textbf{Automata-based systems}. \emph{Automata-based systems} typically propose a CER query language that is inspired by regular expressions which are evaluated by custom automata models. Our work has been inspired by CORE \cite{core}, and falls in the class of automata-based systems. Previous proposals (like SASE \cite{sase}) do not provide \emph{denotational semantics} for their language. As a result, either iterations cannot be nested \cite{skip-till-any-match}, or their semantics is confusing \cite{next-cep}. Other proposals (like TESLA \cite{tesla}) have formal semantics, but they do not include the iteration operator. An exception is Cayuga \cite{cayuga},  but their sequencing operator is non-associative, which results in confusing semantics. CORE \cite{core} is the first framework that provides a well-defined formal semantics that is compositional, allowing arbitrary nesting of operators. Moreover, it is the first evaluation algorithm that guarantees, under data complexity, constant time per event and output-linear delay enumeration.

\textbf{Tree-based systems}. \emph{Tree-based systems} \cite{esper, tree-based-system-1, tree-based-system-2} typically consider a CER query language that is inspired by a regular expression. Unlike automata-based system, the queries are evaluated by constructing and evaluating a tree of CER operators. Again, the semantics of queries is not formally defined. Moreover, the evaluation trees do not have formal performance guarantees, even for regular CER queries.

\textbf{Logic-based systems}. \emph{Logic-based systems} \cite{logic-based-system-1, logic-based-system-2, logic-based-system-3} typically express CER queries as rules in some form of logic, e.g., \emph{temporal logic} or \emph{event calculus}, and consequently evaluate CER as logical inference. Consequently, logic-based systems have formal semantics. However, iteration is often expressed by means of recursive rules instead of as an individual operator. Such rule can detect that a CER pattern applies repeatedly, but they do not typically capture the participating events as part of the complex event. Then, the semantics of iteration in logic-based system is different from CEQL.

\textbf{CORE}. \emph{COmplex event Recognition Engine (CORE)} \cite{core} is the first CER engine that circumvents the super-linear partial match problem. It is the first system to propose a representation of partial matches with formal, proven, and optimal performance guarantees. It uses the query language CEQL that has well-defined formal semantics \cite{on-the-expressiveness}. One limitation is that the evaluation algorithm is sequential. Another limitation is that it is restricted to unary CEL.

\section{Stream partitioning}\label{sec:stream-partitioning}

The basic idea of partitioning a stream of events is to split it so that each processing unit receives only a fraction of it. Thus, the goal is to maximize load balance across processing units. This is the main approach in the dataflow graph paradigm.

\textbf{Shuffle}. Also known as round robin, this is the most basic approach for stream partitioning. It consists of blindly routing events to the processing units in a circular fashion. A perfect load balance is achieved, as each processing unit will receive exactly an even fraction of the stream. This is the best approach for stateless operators, as they are executed to individual events, however it will poorly perform for the stateful case where events must be colocated. In such case, the cost of data repartitioning will be extremely high.

\textbf{Field}. Also known as hash, relies on hash functions defined over attributes of the stream to decide to which processing unit to route each event. The most na\"ive approach is to define a key and use it as input to the hash function. This will distribute all events that should be collocated into the same processing unit, and thus stateful operators will have a good performance. However, such settings will greatly fail in the presence of skewness in the used key. To alleviate such constraints, cost-based approaches have been proposed to deal with skewed streams. In order to dynamically adapt to changes in the stream, such methods require to continuously monitor the keys. For instance, \cite{DBLP:journals/vldb/Gedik14} introduces the concept of \textit{load imbalance}. It uses two hash functions, an explicit and a consistent hash, with the goal of minimizing an objective function that combines such load imbalance the state migration cost. To monitor the most frequent keys, the \textit{Lossy Counting} algorithm is used. Similarly, \cite{DBLP:conf/debs/RivettiQABS15} presents the \textit{Distribution-aware Key Grouping} algorithm (DKG). DKG has a learning phase where the heavy hitter keys are detected, using the \textit{Space Saving} algorithm, then in the deployment phase it is used to route events.

\textbf{Partial key grouping}. Partial key grouping approaches lie in the middle between the shuffle and field approaches. They build upon the idea of balanced allocations \cite{DBLP:journals/siamcomp/AzarBKU99}, stating that routing tuples to the least ``full'' (in terms of size) processing unit, out of $d \geq 2$ choices, will highly increase the overall load balancing and thus, performance. \cite{DBLP:conf/icde/NasirMGKS15} presents an approach using two hash functions for the case when $d = 2$, later extended for the case when $d \geq 2$ \cite{load-balancing-2}. In such settings, frequent keys are monitored using the heavy hitter algorithm \textit{Space Saving}. Partial key grouping algorithms avoid sending to the same processing unit skewed keys, and thus avoid the bottleneck generated by hashing approaches. This, however, incurs a cost in data shuffling over the network when events must be colocated to build a state.

\textbf{Network optimization}. Such approaches advocate that the cost of moving data over the network to colocate tuples that build a state (i.e., aggregation cost) should be considered as first class citizen in partitoning algorithms, and thus minimized. In \cite{DBLP:journals/pvldb/KatsipoulakisLC17}, the authors propose to track the aggregation cost by counting the number of distinct keys sent to each processing unit, achieved via the \textit{HyperLogLog} algorithm. Then, multiple hash functions are used combining the load imbalance and aggregation cost. A different approach is the one presented in \cite{DBLP:conf/middleware/CaneillELP16}, where the authors propose to find correlations in used in subsequent operators and colocate them in the same processing instance. This is achieved by maintaining statistics on the distribution of keys across operators. Then, for each pair of subsequent stateful operators, a bipartite graph is built where nodes represent each distinct key and its weight, and edges the correlations. Thus, the problem is reduced to a graph partitioning problem such that pairs of keys that are highly correlated are in the same group (i.e., same processing unit).

\section{Distributed CER}\label{sec:distributed-cer}

\emph{Distributed CER systems} typically propose to increase the throughput by distributing the workload into a cluster of machines. Several distributed CER systems have been previously proposed \cite{esper, flink-cep, next-cep, distributed-related-work-1, distributed-related-work-2}, however, they do not usually have a well-defined computational model with clear performance guarantees \cite{distributed-related-work-1}, and they do usually suffer from communication overhead and require complex heuristics to optimize performance \cite{distributed-related-work-2}

\textbf{Query partitioning}. These approaches deal with an automata-based computational model for CER. The idea behind query partitioning is to replicate an instance of the automaton to each processing unit. Now, however, each incoming event is a candidate to trigger any automata transition. Thus, each instance of the automata will receive all events but disregard most of them. This approach can be combined with hashing, if one can define a key for the events. This is the solution presented in \cite{DBLP:conf/debs/BrennaGHJ09} in the context of the Cayuga system \cite{DBLP:conf/sigmod/BrennaDGHOPRTW07}. Combined with hashing, \cite{DBLP:conf/debs/Hirzel12} also presents an approach to deal with query partitioning with a custom CER language built on top of IBM's System~S \cite{DBLP:journals/toplas/Hirzel0G17}. \cite{DBLP:conf/debs/BalkesenDWT13} presents a fine-grained approach to query partitioning, where besides providing a \texttt{PARTITION BY} operator that performs hashing, they also parallelize different runs of the automaton. To ensure correctness when routing the events to the active runs, queries are restricted with a \texttt{MAXLENGTH} parameter and batches of events smaller with a cardinality smaller than such parameter are sent to the active run.

\textbf{Pipelining}. The pipelining approach builds on the idea that every NFA with at least one forward edge can be split into smaller automata running on separate processing units \cite{DBLP:conf/debs/BrennaGHJ09}. Then, each processing unit will take care of a partition of the automaton, and there will be special transitions that span across processing units. Similarly as before, since an incoming event can cause state transitions, each processing unit must receive every event.

\section{Chapter summary}

In this chapter, we presented the work related to our research. First, we described the three categories of CER systems and introduced the CORE engine. Then, we discussed several approaches for stream partitioning. Finally, we describe existing approaches to distributed CER and their limitations.
