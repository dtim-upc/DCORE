\chapter{Related Work}\label{chapter:related_work}

In this chapter we describe the state in the literature of the fields related to our work. These fields include: stream partitioning, CER systems, and \emph{distributed} CER systems.

\section{Stream partitioning}\label{sec:stream-partitioning}

The basic idea of partitioning a stream of events is to split it so that each processing unit receives only a fraction of it. Thus, the goal is to maximize load balance across processing units. This is the main approach in the dataflow graph paradigm.

\textbf{Shuffle}. Also known as round robin, this is the most basic approach for stream partitioning. It consists of blindly routing events to the processing units in a circular fashion. A perfect load balance is achieved, as each processing unit will receive exactly an even fraction of the stream. This is the best approach for stateless operators, as they are executed to individual events, however it will poorly perform for the stateful case where events must be colocated. In such case, the cost of data repartitioning will be extremely high.

\textbf{Field}. Also known as hash, relies on hash functions defined over attributes of the stream to decide to which processing unit to route each event. The most na\"ive approach is to define a key and use it as input to the hash function. This will distribute all events that should be collocated into the same processing unit, and thus stateful operators will have a good performance. However, such settings will greatly fail in the presence of skewness in the used key. To alleviate such constraints, cost-based approaches have been proposed to deal with skewed streams. In order to dynamically adapt to changes in the stream, such methods require to continuously monitor the keys. For instance, \cite{DBLP:journals/vldb/Gedik14} introduces the concept of \textit{load imbalance}. It uses two hash functions, an explicit and a consistent hash, with the goal of minimizing an objective function that combines such load imbalance the state migration cost. To monitor the most frequent keys, the \textit{Lossy Counting} algorithm is used. Similarly, \cite{DBLP:conf/debs/RivettiQABS15} presents the \textit{Distribution-aware Key Grouping} algorithm (DKG). DKG has a learning phase where the heavy hitter keys are detected, using the \textit{Space Saving} algorithm, then in the deployment phase it is used to route events.

\textbf{Partial key grouping}. Partial key grouping approaches lie in the middle between the shuffle and field approaches. They build upon the idea of balanced allocations \cite{DBLP:journals/siamcomp/AzarBKU99}, stating that routing tuples to the least ``full'' (in terms of size) processing unit, out of $d \geq 2$ choices, will highly increase the overall load balancing and thus, performance. \cite{DBLP:conf/icde/NasirMGKS15} presents an approach using two hash functions for the case when $d = 2$, later extended for the case when $d \geq 2$ \cite{load-balancing-2}. In such settings, frequent keys are monitored using the heavy hitter algorithm \textit{Space Saving}. Partial key grouping algorithms avoid sending to the same processing unit skewed keys, and thus avoid the bottleneck generated by hashing approaches. This, however, incurs a cost in data shuffling over the network when events must be colocated to build a state.

\textbf{Network optimization}. Such approaches advocate that the cost of moving data over the network to colocate tuples that build a state (i.e., aggregation cost) should be considered as first class citizen in partitoning algorithms, and thus minimized. In \cite{DBLP:journals/pvldb/KatsipoulakisLC17}, the authors propose to track the aggregation cost by counting the number of distinct keys sent to each processing unit, achieved via the \textit{HyperLogLog} algorithm. Then, multiple hash functions are used combining the load imbalance and aggregation cost. A different approach is the one presented in \cite{DBLP:conf/middleware/CaneillELP16}, where the authors propose to find correlations in used in subsequent operators and colocate them in the same processing instance. This is achieved by maintaining statistics on the distribution of keys across operators. Then, for each pair of subsequent stateful operators, a bipartite graph is built where nodes represent each distinct key and its weight, and edges the correlations. Thus, the problem is reduced to a graph partitioning problem such that pairs of keys that are highly correlated are in the same group (i.e., same processing unit).

\section{CER systems}\label{sec:cer-systems}

In this section we describe the three groups that usually divide CER systems. In particular, we focus on the automaton-based system introduced in Chapter~1, CORE \cite{core}. We situate CORE in the literature, and compare it against other state-of-the-art CER systems. We focus on CORE because it is the engine that we will be using in the implementation of our distributed CER framework and our distributed evaluation algorithm.

\textbf{Automata-based systems}. \emph{Automata-based systems} usually propose a query language based on regular expressions that is evaluated by a finite-state machine. There has been many proposals based on automaton systems \cite{survey-systems-1,survey-systems-2}, but either they do not provide \emph{denotational semantics} for their language (e.g, SASE \cite{sase}) or they do not include the iteration (i.e., \emph{kleene plus}) operator (e.g., TESLA \cite{tesla}). CORE \cite{core} is the first framework that provides a well-defined formal semantics that is compositional, allowing arbitrary nesting of operators. Moreover, it is the first evaluation algorithm that guarantees, under data complexity, constant time per event and output-linear delay enumeration.

\textbf{Tree-based systems}. \emph{Tree-based systems} usually propose a query language based on regular expression. Unlike automata-based system, the queries are evaluated by constructing and evaluating a tree of CER operators \cite{tree-based-system-1, tree-based-system-2}. The main caveat of tree-based systems is that their semantics is usually not well defined and they do not usually provide performance guarantees.

\textbf{Logic-based systems}. \emph{Logic-based systems} usually express queries as rules in some form of logic and evaluate CER queries using logical inference \cite{logic-based-system-1, logic-based-system-2} . In contrast the other approaches, logic-based systems have formal semantics. The main caveat of logic-based systems is that they evaluate iterations in terms of recursive inference rules which semantics is different from automaton-based and tree-based CER systems.

\section{Distributed CER}\label{sec:distributed-cer}

\emph{Distributed CER systems} typically propose to increase the throughput by distributing the workload into a cluster of machines. Several distributed CER systems have been previously proposed \cite{esper, flink-cep, next-cep, distributed-related-work-1, distributed-related-work-2}, however, they do not usually have a well-defined computational model with clear performance guarantees \cite{distributed-related-work-1}, and they do usually suffer from communication overhead and require complex heuristics to optimize performance \cite{distributed-related-work-2}

\textbf{Query partitioning}. These approaches deal with an automata-based computational model for CER. The idea behind query partitioning is to replicate an instance of the automaton to each processing unit. Now, however, each incoming event is a candidate to trigger any automata transition. Thus, each instance of the automata will receive all events but disregard most of them. This approach can be combined with hashing, if one can define a key for the events. This is the solution presented in \cite{DBLP:conf/debs/BrennaGHJ09} in the context of the Cayuga system \cite{DBLP:conf/sigmod/BrennaDGHOPRTW07}. Combined with hashing, \cite{DBLP:conf/debs/Hirzel12} also presents an approach to deal with query partitioning with a custom CER language built on top of IBM's System~S \cite{DBLP:journals/toplas/Hirzel0G17}. \cite{DBLP:conf/debs/BalkesenDWT13} presents a fine-grained approach to query partitioning, where besides providing a \texttt{PARTITION BY} operator that performs hashing, they also parallelize different runs of the automaton. To ensure correctness when routing the events to the active runs, queries are restricted with a \texttt{MAXLENGTH} parameter and batches of events smaller with a cardinality smaller than such parameter are sent to the active run.

\textbf{Pipelining}. The pipelining approach builds on the idea that every NFA with at least one forward edge can be split into smaller automata running on separate processing units \cite{DBLP:conf/debs/BrennaGHJ09}. Then, each processing unit will take care of a partition of the automaton, and there will be special transitions that span across processing units. Similarly as before, since an incoming event can cause state transitions, each processing unit must receive every event.

\section{Chapter summary}

In this chapter, we presented the work related to our research. First, we described the three categories of CER systems and introduced the CORE engine. Then, we discussed several approaches for stream partitioning. Finally, we describe existing approaches to distributed CER and their limitations.
